from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
from langchain_chroma import Chroma

from langchain.agents.middleware import dynamic_prompt, ModelRequest
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain_huggingface import HuggingFaceEmbeddings


load_dotenv()


# Embedding area
model_name = "DeepVk/USER-bge-m3"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': True}

embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs,
    cache_folder="./transformers_models"
)

# –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–∫–∞–∂–∏—Ç–µ –ø–æ–ª–Ω–æ–µ –∏–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º pipeline_id
vector_store = Chroma(
    collection_name="example_collection_b2be69b0",  # ‚Üê –ò–ó–ú–ï–ù–ï–ù–û!
    embedding_function=embeddings,
    persist_directory="./chroma_langchain_db/b2be69b0",
)

# –ü—Ä–æ–≤–µ—Ä–∫–∞: –≤—ã–≤–æ–¥–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î
print(f"üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î: {vector_store._collection.count()}")
print()

# –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤ –∞–≥–µ–Ω—Ç–µ
test_query = "–±–∏–æ–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—ã"
print(f"üîç –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫: '{test_query}'")
test_results = vector_store.similarity_search(test_query, k=3)
print(f"‚úì –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(test_results)}")
if test_results:
    print(f"   –ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {test_results[0].page_content[:100]}...")
print()

# ChatModel Area
model = ChatOpenAI(model="gpt-4o", temperature=0)


@dynamic_prompt
def prompt_with_context(request: ModelRequest) -> str:
    last_query = request.state["messages"][-1].text
    retrieved_docs = vector_store.similarity_search(last_query, k=5)
    
    # DEBUG: –í—ã–≤–æ–¥–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    print(f"üìÑ RAG –Ω–∞—à–µ–ª {len(retrieved_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{last_query}'")

    if not retrieved_docs:
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        docs_content = "–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
    else:
        docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)
        print(f"‚úì –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM: {len(docs_content)} —Å–∏–º–≤–æ–ª–æ–≤")

    system_message = (
        "You are RAG system that should answer user prompt using this information:"
        f"\n\n{docs_content}"
    )
    return system_message

agent = create_agent(model, tools=[], middleware=[prompt_with_context])

query = "–ö–∞–∫–∏–µ —Ç–∏–ø—ã —É—Å–∏–ª–∏—Ç–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –±–∏–æ–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–≤"

print("="*60)
print(f"‚ùì –í–æ–ø—Ä–æ—Å: {query}")
print("="*60)
print()

response = agent.invoke({"messages": [{"role": "user", "content": query}] })

print("="*60)
print("üí¨ –û—Ç–≤–µ—Ç:")
print("="*60)
print(response["messages"][-1].content)